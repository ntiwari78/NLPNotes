

## üìò Study Guide: N-gram Language Models

---

### üîç Core Concepts

* **Definition**

  * A language model predicts upcoming words and assigns probabilities to sequences of words (sentences).

* **Applications**

  * **Spell Checking**: Detects improbable word sequences.
  * **Speech Recognition**: Ranks word hypotheses based on context.
  * **Machine Translation**: Evaluates fluency of output sentences.

* **Relation to LLMs**

  * Used in **auto-regressive generation**.
  * Forms the basis for **pre-training objectives**.
  * Supports **self-supervised learning** frameworks.

---

### üî¢ Probabilistic Foundations

* **Chain Rule**

  * Decomposes joint probabilities into products of conditional probabilities.
  * Enables stepwise prediction based on prior context.

* **Markov Assumption**

  * Assumes only a limited history is relevant (fixed window size).
  * Helps approximate full sentence probabilities with manageable complexity.

---


## üìò **Study Guide: N-gram Language Models**

This guide summarizes key concepts, provides short-answer questions with an answer key, offers essay prompts for deeper understanding, and includes a glossary of important terms.

---

### ‚úÖ **Quiz: Short-Answer Questions**

**1. What are the two primary tasks a language model is designed to perform?**
Language models predict the next word in a sequence and assign probabilities to entire sentences. This helps in evaluating sentence plausibility.

**2. Explain how language modeling is used in context-sensitive spelling correction.**
The model assigns probabilities to alternate versions of a sentence with different corrections. The version with the highest overall sentence probability is chosen as the most likely intended sentence.

**3. What is the fundamental relationship between the language modeling objective and the training of LLMs?**
LLMs are trained using the language modeling objective‚Äîpredicting the next word in a sequence. This pretraining task is auto-regressive and self-supervised.

**4. What is the Chain Rule of Probability, and how is it used to calculate the probability of a sentence?**
It decomposes the joint probability of a word sequence into a product of conditional probabilities. Each word's probability is conditioned on all previous words in the sentence.

**5. What practical difficulty arises when trying to estimate the probabilities required by the chain rule for a long sentence?**
Long sequences are rare in training data, making it difficult to estimate probabilities accurately. This issue is known as **data sparsity**.

**6. Explain the Markov assumption and how it simplifies the calculation of word probabilities.**
It assumes that the probability of a word depends only on a fixed number of previous words. For example, a bigram model assumes each word depends only on the one before it.

**7. Compare the quality of text generated by a unigram model versus a quadgram model.**
A unigram model generates incoherent text, as it selects words independently. A quadgram model considers three previous words and produces more grammatically and contextually coherent text.

**8. Identify two major limitations of N-gram language models.**
They struggle with long-distance dependencies and require exponentially more parameters as N increases, leading to inefficient scaling.

**9. According to the Maximum Likelihood Estimate (MLE), how would you calculate the probability of a word (wi) given the previous word (wi-1)?**
By dividing the count of the bigram (wi-1, wi) by the count of the prefix (wi-1):
P(wi | wi-1) = Count(wi-1, wi) / Count(wi-1).

**10. What is the numerical "underflow" problem when calculating sentence probability, and what is the standard solution?**
Multiplying many small probabilities results in a number too small for computers to represent accurately. The solution is to compute in **log space**, converting multiplication into addition.

---

### üß† **Essay Questions**

1. **Evolution of N-gram Text Generation**
   Discuss improvements from unigram to quadgram models, using examples (e.g., Shakespeare generation). Explain how larger N captures more context.

2. **Pre-training and Prompting in LLMs**
   Define both concepts. How does the next-word prediction task during pre-training relate to how LLMs generate and respond to prompts?

3. **Long-Distance Dependencies**
   Use the ‚Äúsoups...were delicious‚Äù example to explain why N-gram models can‚Äôt capture subject‚Äìverb agreement across distant words. Connect this to the parameter explosion problem.

4. **Bigram Sentence Probability Calculation**
   Start with the chain rule, apply the bigram Markov assumption, explain MLE estimation, and describe the shift to log probabilities to avoid underflow.

5. **Language Models in Speech Recognition**
   Describe how LMs evaluate possible transcriptions by assigning sentence-level probabilities and selecting the most probable one (e.g., disambiguating ‚ÄúI will be Bish‚Äù).

---

### üìö **Glossary of Key Terms**

| **Term**                              | **Definition**                                                                        |
| ------------------------------------- | ------------------------------------------------------------------------------------- |
| **Language Modeling (LM)**            | Predicts the next word in a sequence; assigns sentence-level probabilities.           |
| **N-gram Model**                      | Predicts a word based on the previous n‚àí1 words (Markov assumption).                  |
| **Auto-regressive Model**             | Predicts one word at a time using past context (e.g., GPT models).                    |
| **Pre-training**                      | Initial training phase using self-supervised learning.                                |
| **Prompting**                         | Technique for guiding pre-trained models by formatting input with examples and tasks. |
| **Chain Rule of Probability**         | Decomposes a joint probability into conditional probabilities.                        |
| **Markov Assumption**                 | Assumes a word depends only on a fixed number of prior words.                         |
| **Unigram Model**                     | Assumes each word is independent of others.                                           |
| **Bigram Model**                      | Assumes each word depends only on the immediately previous word.                      |
| **Trigram Model**                     | Considers the two previous words for prediction.                                      |
| **Quadgram Model**                    | Uses the last three words to predict the next, producing more fluent output.          |
| **Long-distance Dependencies**        | Grammatical relationships between words far apart in a sentence.                      |
| **Maximum Likelihood Estimate (MLE)** | Probability estimated via relative frequency counts.                                  |
| **Underflow**                         | Error from multiplying small probabilities‚Äîvalues become too small to represent.      |
| **Log Space**                         | Mathematical domain where log values are used to prevent underflow.                   |

---

### üßÆ Types of N-gram Models

| Model Type           | Description                                         |
| -------------------- | --------------------------------------------------- |
| **Unigram (N=1)**    | Words are treated as independent of context.        |
| **Bigram (N=2)**     | Considers 1 preceding word (context window = 1).    |
| **Trigram (N=3)**    | Considers 2 preceding words.                        |
| **Quadrigram (N=4)** | Considers 3 preceding words; yields more coherence. |

---

### üß† Practical Estimation

* **Maximum Likelihood Estimation (MLE)**

  * Based on **relative frequency counts**.
  * Probabilities are normalized by prefix counts (e.g., P(w3 | w1 w2) = Count(w1 w2 w3) / Count(w1 w2)).

* **Implementation Issues**

  * **Numerical Underflow**: Multiplying many small probabilities leads to instability.
  * **Solution**: Use log-space addition to stabilize computations.
  * **Data Sparsity**: Many N-gram combinations never occur in training data.

---

### ‚ö†Ô∏è Limitations

* **Long-Distance Dependencies**

  * N-grams struggle to model relationships between distant words (e.g., subject‚Äìverb agreement across clauses).

* **Parameter Explosion**

  * As N increases, the number of possible N-grams grows exponentially.

* **Lack of Synonym Generalization**

  * N-grams memorize specific word sequences, failing to recognize that "car" and "automobile" are semantically similar.

---
