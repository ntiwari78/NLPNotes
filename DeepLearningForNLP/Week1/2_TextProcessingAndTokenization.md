

# ğŸ“˜ Study Guide: Text Processing Basics and Tokenization

---

## ğŸ§± 1. Basics of Text Processing

### ğŸ”¹ Identification of Units

* **Tokens**: Individual units (words, punctuation, etc.)
* **Types (Unique Words)**: Distinct tokens
* **Vocabulary**: Set of all types in a text

### ğŸ”¹ Heapsâ€™ Law

* Describes vocabulary growth
* Relationship: **V (types)** vs. **N (tokens)**

### ğŸ”¹ Corpus Variations

* **Language Diversity**
* **Abbreviations / Code-Switching**
* **Genre & Domain Knowledge**

---

## âœ‚ï¸ 2. Traditional Tokenization

### ğŸ”¹ White Space Tokenizer

* **Common in English**
* **Challenges**: Clitics, punctuation

### ğŸ”¹ Handling Unseen Words

* **Out-of-Vocabulary (OOV)**
* **Unknown Token (Unk)**: Problems with oversimplification

### ğŸ”¹ Language-Specific Challenges

* **Chinese Segmentation**
* **Sanskrit Sandhi Rules**
* **Morphologically Rich Languages**

---

## ğŸ”„ 3. Other Text Processing Steps

* **Lemmatization**
* **Stemming**
* **Sentence Segmentation**
* **Stop Word Removal**
* **Casing (Lowercasing)**

---

## ğŸ§  4. Modern Tokenization (LLMs)

### ğŸ”¹ Subword Tokenization

* **Morpheme-based units**
* **Pre-tokenization vs Tokenization**

### ğŸ”¹ Byte Pair Encoding (BPE)

* **Token Learner Phase**
* **Token Segmenter Phase**
* **Iterative Merging**

### ğŸ”¹ Alternative Algorithms

* **WordPiece**
* **SentencePiece**

### ğŸ”¹ LLM Implementations

* **Models**: GPT-2/3/4, LLaMA 2
* **Vocabulary Sizes**: 30kâ€“100k tokens

---

## âš ï¸ 5. Implications

* **Semantic Loss in Subwords**
* **Token-based Cost Disparity**
* **Language Frequency Bias**

