

# Natural Language Processing â€“ Course Introduction

**Instructor:** Prof. Pawan Goyal
**Institution:** [IIT Kharagpur](https://www.iitkgp.ac.in/)

---

## ğŸ“˜ Course Overview

* **Duration:** 12 weeks
* **Structure:** 5 modules per week
* **Support:** Two TAs â€“ Amrith Krishna and Mayank Singh
* **Contact:** Provided in the course materials

---

## ğŸ“š Recommended Books

1. **[Speech and Language Processing](https://web.stanford.edu/~jurafsky/slp3/)** by Jurafsky and Martin (2nd or 3rd edition)
2. **[Foundations of Statistical Natural Language Processing](https://mitpress.mit.edu/9780262133609/foundations-of-statistical-natural-language-processing/)** by Manning and SchÃ¼tze

Additional materials and [IPython Notebooks](https://jupyter.org/) will be shared for hands-on learning.

---

## ğŸ§ª Evaluation Scheme

* **Assignments:** 25% (weekly)
* **Final Exam:** 75%

---

## ğŸ“Œ Topics Covered

### Basic Topics

* **Text Processing:** Tokenization, stemming, lemmatization
* **[Language Modeling](https://en.wikipedia.org/wiki/Language_model)**
* **Morphology & Syntax**
* **[Semantics](https://en.wikipedia.org/wiki/Semantics):** Lexical and distributional semantics
* **[Word Embeddings](https://en.wikipedia.org/wiki/Word_embedding)**
* **[Topic Modeling](https://en.wikipedia.org/wiki/Topic_model)**

### Applications

* **[Entity Linking](https://en.wikipedia.org/wiki/Entity_linking)** & **[Information Extraction](https://en.wikipedia.org/wiki/Information_extraction)**
* **[Text Summarization](https://en.wikipedia.org/wiki/Automatic_summarization)**
* **[Text Classification](https://en.wikipedia.org/wiki/Text_classification)**
* **[Sentiment Analysis](https://en.wikipedia.org/wiki/Sentiment_analysis)**

---

## ğŸ§  NLP: Scientific vs Engineering Goals

* **Scientific:** Can machines truly understand human language?
* **Engineering:** Build practical tools (e.g., [Google Translate](https://translate.google.com/), search engines, chatbots)

---

## ğŸŒ Why Study NLP?

* Text is the largest store of human knowledge ([Wikipedia](https://www.wikipedia.org/), news, scientific papers, [social media](https://en.wikipedia.org/wiki/Social_media))
* Available in many languages: multilingual processing and [machine translation](https://en.wikipedia.org/wiki/Machine_translation) are essential

---

## ğŸ§© Challenges in NLP

### Ambiguities

* **Lexical Ambiguity:** e.g., *â€œWill Will will Willâ€™s will?â€*
* **Structural Ambiguity:** e.g., *â€œThe man saw the boy with the binocularsâ€*
* **Vagueness:** e.g., *â€œItâ€™s very warmâ€*

### Social Media & Informal Text

* **Non-standard language:** *CU L8R*, hashtags, emojis
* **New words/senses:** e.g., *Googling*, *unfriending*

### Other Complexities

* **[Idioms](https://en.wikipedia.org/wiki/Idiom)**
* **[Named Entity Recognition](https://en.wikipedia.org/wiki/Named-entity_recognition)**
* **Multilingual content**
* **Discourse context**

---

## ğŸ“ˆ Empirical Laws in Language

### Word Frequency

* Dominated by **function words** (e.g., *the*, *is*)
* Occasionally topic-specific **content words** (e.g., *Tom* in *Tom Sawyer*)

### Type-Token Ratio (TTR)

* **TTR = Unique Words / Total Words**
* Indicates richness and repetitiveness of vocabulary
* Varies by genre:

  * High TTR: academic texts
  * Low TTR: casual conversation

---

## ğŸ“ Tools & Approaches

* **[Probabilistic Models](https://en.wikipedia.org/wiki/Statistical_natural_language_processing)**
* **[Parsing](https://en.wikipedia.org/wiki/Parsing)**
* **[Machine Learning](https://en.wikipedia.org/wiki/Machine_learning)**
* **Language-specific rules and corpora**

---

## References

* [Jurafsky & Martin â€“ Speech and Language Processing](https://web.stanford.edu/~jurafsky/slp3/)
* [Manning & SchÃ¼tze â€“ Foundations of Statistical NLP](https://mitpress.mit.edu/9780262133609/foundations-of-statistical-natural-language-processing/)
* [Natural Language Processing (Wikipedia)](https://en.wikipedia.org/wiki/Natural_language_processing)
* [Word Embedding](https://en.wikipedia.org/wiki/Word_embedding)
* [Topic Modeling](https://en.wikipedia.org/wiki/Topic_model)
* [Named Entity Recognition](https://en.wikipedia.org/wiki/Named-entity_recognition)
* [Machine Learning](https://en.wikipedia.org/wiki/Machine_learning)



# Natural Language Processing â€“ Lecture 2

## ğŸ¯ What Do We Do in NLP?

In this lecture, Prof. Pawan Goyal explores the real-world tasks and applications tackled in **Natural Language Processing (NLP)**, emphasizing both its ambitious and practical goals.

---

## ğŸ“ Goals of NLP

### Scientific Goal

* Understand how **humans process language**
* Teach **computers** to understand and respond in **natural language**

### Engineering Goal

* Build systems that **process language** for **practical use cases**
* Examples: Translation, summarization, search engines, chatbots

---

## âš™ï¸ Ambitious Applications

### 1. **Machine Translation**

* Tools like [Google Translate](https://translate.google.com/) are widely used but **not always accurate**
* **Word Sense Disambiguation (WSD)** is crucial (e.g., "cool" â‰  "cold")

### 2. **Conversational Agents**

* Open-domain chatbots (e.g., Microsoftâ€™s [Tay](https://en.wikipedia.org/wiki/Tay_%28bot%29)) failed due to lack of control
* Domain-specific bots (e.g., course assistants) are more successful

---

## âœ… Practical Applications

### ğŸ” Information Retrieval & Query Processing

* **Spelling correction**
* **Query completion** using [language models](https://en.wikipedia.org/wiki/Language_model)

### ğŸ§  Information Extraction

* Extract structured facts (e.g., names, roles, dates) from **unstructured text**
* Applications in [Named Entity Recognition (NER)](https://en.wikipedia.org/wiki/Named-entity_recognition)

### ğŸ¤– Educational Assistants

* Use of chatbots in courses to answer routine queries with high accuracy

### ğŸ—£ï¸ Sentiment Analysis

* Analyzing opinions from [social media](https://en.wikipedia.org/wiki/Social_media), reviews, political discourse

### ğŸš« Spam Detection

* Filters in email and platforms like YouTube and Twitter
* Classify based on **textual patterns**

### ğŸŒ Machine Translation Services

* Translating full webpages and documents

### ğŸ“° Text Summarization

* Creating concise summaries from news or scientific articles

---

## ğŸ›  Challenges in NLP Applications

* Systems can make **blunders**, especially when:

  * Lacking **contextual understanding**
  * Working in **open domains**

* Yet NLP is good enough for:

  * **Search engines**
  * **Assistants**
  * **Language services**

---

## ğŸ“Œ Summary

NLP includes both high-level research and grounded engineering. Its ultimate goal is to:

* Build intelligent systems that **understand and process human language**
* Create real-world tools that solve practical problems

---

## References

* [Google Translate](https://translate.google.com/)
* [Word Sense Disambiguation](https://en.wikipedia.org/wiki/Word-sense_disambiguation)
* [Microsoft Tay Bot](https://en.wikipedia.org/wiki/Tay_%28bot%29)
* [Language Models](https://en.wikipedia.org/wiki/Language_model)
* [Named Entity Recognition](https://en.wikipedia.org/wiki/Named-entity_recognition)
* [Sentiment Analysis](https://en.wikipedia.org/wiki/Sentiment_analysis)
* [Text Summarization](https://en.wikipedia.org/wiki/Automatic_summarization)




# Natural Language Processing â€“ Lecture 3

## ğŸ¤” Why is NLP Hard?

This lecture explores the inherent complexities and ambiguities in natural languages that make **NLP a challenging field**. It discusses types of ambiguities, linguistic phenomena, and why language understanding is far from trivial.

---

## ğŸ”„ Types of Ambiguity in Language

### 1. **Lexical Ambiguity**

* A word has multiple meanings.

* Example: *â€œWill Will will Willâ€™s will?â€*

  * Modal verb, proper noun, future action, noun (legal document)

* Example: *â€œRose rose to put rose roes on her rows of roses.â€*

* Famous: *â€œBuffalo buffalo Buffalo buffalo buffalo buffalo Buffalo buffalo.â€*

  * Uses "buffalo" as city, animal, and verb (to bully)

[Lexical Ambiguity â†’ Wikipedia](https://en.wikipedia.org/wiki/Lexical_ambiguity)

---

### 2. **Structural Ambiguity**

* Different parse trees yield different meanings.

* Example: *â€œThe man saw the boy with the binoculars.â€*

  * Who has the binoculars?

* Example: *â€œFlying planes can be dangerous.â€*

  * Flying is dangerous OR flying planes are dangerous?

[Structural Ambiguity â†’ Wikipedia](https://en.wikipedia.org/wiki/Syntactic_ambiguity)

---

## ğŸ§© Other Language Complexities

### ğŸ” Vagueness and Imprecision

* Example: *â€œItâ€™s very warm here.â€*

  * What temperature is â€œwarmâ€?

* Example: *â€œIâ€™m sure she must have.â€*

  * Indicates uncertainty

---

## ğŸ˜‚ Ambiguity in Humor

* Jokes often rely on ambiguity:

  * *â€œWhy is the teacher wearing sunglasses?â€ â†’ â€œBecause the class is bright.â€*

---

## ğŸ—ï¸ Ambiguity in Headlines

* Example: *â€œHospitals are sued by 7 foot doctors.â€*
* Example: *â€œStolen painting found by tree.â€*
* Example: *â€œTeacher strikes idle kids.â€*

---

## ğŸ§  Exercise: "I Made Her Duck"

Find multiple interpretations:

1. I cooked a duck for her.
2. I cooked the duck that belongs to her.
3. I created a toy duck for her.
4. I forced her to lower her head.
5. I turned her into a duck (magic).

Ambiguity arises due to:

* **Syntactic Category Ambiguity** (noun vs. verb)
* **Possessive vs. Dative Interpretation** of â€œherâ€
* **Verb Usage**: transitive, ditransitive, action-transitive
* **Speech Ambiguity** (e.g., â€œIâ€™m aid her duckâ€ vs. â€œI made her duckâ€)

---

## ğŸ§® Parsing Explosion

* Sentence: *â€œI saw the man on the hill in Texas with the telescopeâ€¦â€*
* Number of parses increases rapidly with sentence length

  * Example: 132 parses for one sentence (relates to [Catalan numbers](https://en.wikipedia.org/wiki/Catalan_number))

---

## ğŸ—£ï¸ Why Is Language Ambiguous?

* **Efficiency:** Humans prefer concise expressions.
* **Shared Knowledge:** Listeners resolve ambiguity using context.
* NLP systems lack this shared background knowledge.

---

## ğŸ¤– Natural vs. Programming Languages

| Feature      | Natural Language            | Programming Language     |
| ------------ | --------------------------- | ------------------------ |
| Ambiguity    | High                        | None                     |
| Grammar      | Implicit, context-dependent | Explicit, formal         |
| Parsing Time | Variable, non-deterministic | Deterministic, efficient |

---

## ğŸ§µ Challenges in Modern Text (e.g., Social Media)

* Non-standard forms: *CU L8R*, @mentions, hashtags
* [Code-switching](https://en.wikipedia.org/wiki/Code-switching), new slang
* Ambiguous segmentation (e.g., â€œNew York-New Haven Railroadâ€)
* Idioms: e.g., *â€œburn the midnight oilâ€*
* Evolving usage: *â€œunfriendâ€*, *â€œretweetâ€*, *â€œGoogleâ€* as a verb

---

## ğŸ“ NLP Requires

* Knowledge of:

  * **Language structure**
  * **World knowledge**
  * **Efficient integration methods**

* **Probabilistic models** are used to:

  * Handle ambiguity
  * Predict word meaning and structure
  * Support applications like [speech recognition](https://en.wikipedia.org/wiki/Speech_recognition)

---

## References

* [Lexical Ambiguity](https://en.wikipedia.org/wiki/Lexical_ambiguity)
* [Syntactic Ambiguity](https://en.wikipedia.org/wiki/Syntactic_ambiguity)
* [Catalan Numbers](https://en.wikipedia.org/wiki/Catalan_number)
* [Speech Recognition](https://en.wikipedia.org/wiki/Speech_recognition)
* [Code-Switching](https://en.wikipedia.org/wiki/Code-switching)
* [Idioms in Language](https://en.wikipedia.org/wiki/Idiom)



# Natural Language Processing â€“ Lecture 4

## ğŸ“Š Empirical Laws in Language

In this lecture, Prof. Pawan Goyal introduces empirical linguistic patterns found in real-world corpora, focusing on **word distributions**, **types vs. tokens**, and the distinction between **function** and **content** words.



## ğŸ§± Function Words vs Content Words

### ğŸ“Œ Function Words

* Serve **grammatical roles** (e.g., *the*, *is*, *and*, *to*)
* Include **prepositions**, **pronouns**, **auxiliary verbs**, **conjunctions**, and **articles**
* Form a **closed class** (few new entries)

### ğŸ”¤ Content Words

* Carry **semantic meaning**: **nouns**, **verbs**, **adjectives**, etc.
* Form an **open class** (new words regularly added)

ğŸ“– Related: [Function and Content Words (Wikipedia)](https://en.wikipedia.org/wiki/Function_word)



## ğŸ§ª Demonstration: Word Substitution

Two modified sentences were presented:

* One with **content words replaced** (meaning lost, structure visible)
* One with **function words replaced** (structure distorted, meaning retained)

**Conclusion:**

* **Function words** provide **syntactic structure**
* **Content words** convey **topic and meaning**



## ğŸ“š Word Frequencies in a Corpus

Corpus: *Tom Sawyer* by [Mark Twain](https://en.wikipedia.org/wiki/Mark_Twain)

### Top Frequent Words:

* â€œtheâ€ â€“ 3332 times
* â€œandâ€, â€œtoâ€, â€œaâ€, â€œofâ€ â€“ all high frequency
* Mostly **function words**

### Notable Exception:

* **â€œTomâ€** appears frequently due to the topic of the book

ğŸ”— [Word Frequency](https://en.wikipedia.org/wiki/Word_frequency)



## ğŸ”  Type vs Token

### Definitions:

| Term      | Meaning                                                |
| --------- | ------------------------------------------------------ |
| **Token** | Each occurrence of a word in the corpus                |
| **Type**  | Unique word (distinct spelling/form) in the vocabulary |

> E.g., "will will" â†’ 2 tokens, 1 type



## ğŸ“ Type-Token Ratio (TTR)

**TTR = Unique Words (Types) / Total Words (Tokens)**

* **High TTR:** Many unique words, diverse vocabulary
* **Low TTR:** Repetitive usage

### Corpus Comparison:

| Text                   | Tokens | Types  | TTR   |
| ---------------------- | ------ | ------ | ----- |
| *Tom Sawyer*           | 71,370 | 8,018  | 0.112 |
| *Complete Shakespeare* | 88,400 | 29,066 | 0.329 |

ğŸ”— [Type-Token Ratio (Wikipedia)](https://en.wikipedia.org/wiki/Lexical_density#Type%E2%80%93token_ratio)



## ğŸ“° TTR by Text Genre

### Genres Compared:

* **Conversation**
* **Academic Prose**
* **News**
* **Fiction**

### Observation:

* **Conversation** tends to have the **lowest TTR** due to word repetition
* **Academic prose** typically has the **highest TTR**

ğŸ“– Related: [Corpus Linguistics](https://en.wikipedia.org/wiki/Corpus_linguistics)



## ğŸ“Œ Summary

* Language exhibits **predictable patterns** in word frequency
* Distinguishing **function vs. content words** is crucial for text analysis
* **TTR** helps measure vocabulary diversity across genres
* Understanding these patterns lays the foundation for **language modeling**, **information retrieval**, and **text classification**



## References

* [Function Word](https://en.wikipedia.org/wiki/Function_word)
* [Word Frequency](https://en.wikipedia.org/wiki/Word_frequency)
* [Type-Token Ratio](https://en.wikipedia.org/wiki/Lexical_density#Type%E2%80%93token_ratio)
* [Corpus Linguistics](https://en.wikipedia.org/wiki/Corpus_linguistics)
* [Mark Twain](https://en.wikipedia.org/wiki/Mark_Twain)



# Natural Language Processing â€“ Lecture 5

## ğŸ“œ Empirical Laws (Continued)

This lecture builds upon Lecture 4, diving deeper into **statistical patterns in language** and their applications in **NLP**. It explores **Zipfâ€™s Law**, **Heapsâ€™ Law**, and **other statistical observations** from linguistic corpora.



## âš–ï¸ Zipfâ€™s Law

### Statement

* In natural language, the **frequency of any word** is **inversely proportional to its rank** in the frequency table.
* Formally:

$$
  f(r) \propto \frac{1}{r}
$$

  where:

  * $f(r)$ = frequency of word with rank $r$
  * $r$ = rank of the word (1 = most frequent)

### Example

* Rank 1 word (*the*) is \~10Ã— more frequent than rank 10 word.
* Observed across many languages and corpora.

ğŸ“– [Zipfâ€™s Law (Wikipedia)](https://en.wikipedia.org/wiki/Zipf%27s_law)



## ğŸ“ˆ Heapsâ€™ Law

### Statement

* As corpus size increases, the **number of unique words (vocabulary size)** grows sublinearly.
* Formula:

$$
  V(N) = kN^\beta
$$

  where:

  * $V(N)$ = number of distinct words in corpus of size $N$
  * $k$, $\beta$ = constants (typically $0.4 < \beta < 0.6$)

### Implication

* Even in massive corpora, new words **keep appearing**, but at a **diminishing rate**.

ğŸ“– [Heapsâ€™ Law (Wikipedia)](https://en.wikipedia.org/wiki/Heaps%27_law)



## ğŸ“Š Statistical Observations

1. **Word Frequencies**:

   * A few words dominate (function words like *the*, *is*, *of*).
   * Most words occur **rarely**.

2. **Vocabulary Growth**:

   * **Infinite potential vocabulary** (due to new words, neologisms, proper names).

3. **Stop Words**:

   * Very frequent but carry little topical meaning.
   * Commonly removed in tasks like **information retrieval**.

ğŸ“– [Stop Words (Wikipedia)](https://en.wikipedia.org/wiki/Stop_words)



## ğŸ§  Applications in NLP

* **Language Modeling** â†’ Predict next word based on probabilities
* **Information Retrieval** â†’ Ignore stop words, focus on content words
* **Text Summarization** â†’ Identify key content words
* **Speech Recognition** â†’ Use frequency models to disambiguate words



## ğŸ“Œ Key Takeaways

* **Zipfâ€™s Law** â†’ Word frequencies follow a predictable power-law distribution
* **Heapsâ€™ Law** â†’ Vocabulary grows with corpus size but sublinearly
* **Implication**: Language is both **repetitive** (common words) and **creative** (new words keep appearing)



## References

* [Zipfâ€™s Law](https://en.wikipedia.org/wiki/Zipf%27s_law)
* [Heapsâ€™ Law](https://en.wikipedia.org/wiki/Heaps%27_law)
* [Stop Words](https://en.wikipedia.org/wiki/Stop_words)
* [Language Modeling](https://en.wikipedia.org/wiki/Language_model)
* [Information Retrieval](https://en.wikipedia.org/wiki/Information_retrieval)


# Natural Language Processing â€“ Lecture 6

## ğŸ›  Text Preprocessing

This lecture introduces **text preprocessing**, the **first step in NLP pipelines**. Preprocessing transforms **raw text** into a structured format suitable for computational models.



## ğŸ§¹ Why Preprocessing?

* Raw text is **noisy, unstructured, and inconsistent**.
* Preprocessing improves:

  * **Efficiency** (smaller vocabulary, reduced redundancy)
  * **Accuracy** (clearer representation of meaning)
  * **Generalization** (removes irrelevant variations)



## ğŸ”‘ Key Preprocessing Steps

### 1. **Tokenization**

* Splitting text into **words**, **sentences**, or **subwords**.
* Challenges:

  * Handling punctuation: *â€œU.S.A.â€ vs â€œUSAâ€*
  * Contractions: *â€œdonâ€™tâ€ â†’ do + not*
  * Languages without spaces: [Chinese word segmentation](https://en.wikipedia.org/wiki/Word_segmentation)

ğŸ“– [Tokenization (Wikipedia)](https://en.wikipedia.org/wiki/Lexical_analysis#Tokenization)



### 2. **Normalization**

* Making text consistent:

  * Lowercasing
  * Removing punctuation
  * Handling numbers (e.g., *123 â†’ NUM*)

ğŸ“– [Text Normalization](https://en.wikipedia.org/wiki/Text_normalization)



### 3. **Stemming**

* Reducing words to their **root form** (often crude, rule-based).
* Example:

  * *running â†’ run*
  * *studies â†’ studi*

ğŸ“– [Stemming](https://en.wikipedia.org/wiki/Stemming)



### 4. **Lemmatization**

* Mapping words to **dictionary form** using linguistic knowledge.
* Example:

  * *better â†’ good*
  * *was â†’ be*

ğŸ“– [Lemmatization](https://en.wikipedia.org/wiki/Lemmatisation)



### 5. **Stop Word Removal**

* Removing very frequent words with little semantic value (e.g., *the, is, of*).
* Helps in tasks like **search engines** and **topic modeling**.

ğŸ“– [Stop Words](https://en.wikipedia.org/wiki/Stop_words)



### 6. **Handling Rare Words / OOV (Out-of-Vocabulary)**

* Replace rare words with a placeholder (e.g., `<UNK>`).
* Alternative: **Subword models** ([Byte Pair Encoding](https://en.wikipedia.org/wiki/Byte_pair_encoding)).



## ğŸ“Š Example Workflow

Sentence: *â€œThe childrenâ€™s studies were running quickly.â€*

1. **Tokenization** â†’ \[The] \[childrenâ€™s] \[studies] \[were] \[running] \[quickly]
2. **Lowercasing** â†’ \[the] \[childrenâ€™s] \[studies] \[were] \[running] \[quickly]
3. **Stemming** â†’ \[the] \[children] \[studi] \[were] \[run] \[quick]
4. **Lemmatization** â†’ \[the] \[child] \[study] \[be] \[run] \[quickly]
5. **Stop word removal** â†’ \[child] \[study] \[run] \[quickly]



## ğŸ§  Why It Matters?

* Preprocessing **directly impacts model performance**.
* Some modern models ([BERT](https://en.wikipedia.org/wiki/BERT_%28language_model%29), [GPT](https://en.wikipedia.org/wiki/GPT-3)) rely on **subword tokenization** rather than heavy preprocessing.
* Choice of preprocessing depends on:

  * **Task requirements**
  * **Model type**
  * **Language specifics**



## ğŸ“Œ Summary

* Text preprocessing is **essential** for NLP tasks.
* Core steps: **tokenization, normalization, stemming, lemmatization, stop-word removal, rare word handling**.
* Modern deep learning models use **subword-based methods** but still rely on preprocessing foundations.



## References

* [Tokenization](https://en.wikipedia.org/wiki/Lexical_analysis#Tokenization)
* [Text Normalization](https://en.wikipedia.org/wiki/Text_normalization)
* [Stemming](https://en.wikipedia.org/wiki/Stemming)
* [Lemmatization](https://en.wikipedia.org/wiki/Lemmatisation)
* [Stop Words](https://en.wikipedia.org/wiki/Stop_words)
* [Byte Pair Encoding](https://en.wikipedia.org/wiki/Byte_pair_encoding)
* [BERT](https://en.wikipedia.org/wiki/BERT_%28language_model%29)
* [GPT Models](https://en.wikipedia.org/wiki/GPT-3)

# Natural Language Processing â€“ Lecture 7

## ğŸ”¢ Edit Distance and String Similarity

This lecture introduces **edit distance** (also called **Levenshtein distance**) and its role in **text similarity**, **spell correction**, and **information retrieval**.


## ğŸ“Œ Motivation

* Natural language text often contains:

  * **Typos**
  * **Spelling variations** (e.g., *color* vs. *colour*)
  * **OCR errors**
* We need a way to **quantify similarity** between two strings.


## âœ‚ï¸ Edit Distance

### Definition

* **Minimum number of operations** required to transform one string into another.
* Allowed operations:

  1. **Insertion**
  2. **Deletion**
  3. **Substitution**

ğŸ“– [Edit Distance (Wikipedia)](https://en.wikipedia.org/wiki/Edit_distance)


### Example

* String 1: **kitten**
* String 2: **sitting**

Operations:

1. kitten â†’ sitten (substitution: k â†’ s)
2. sitten â†’ sittin (substitution: e â†’ i)
3. sittin â†’ sitting (insertion: g)

**Edit Distance = 3**


## ğŸ§® Dynamic Programming Approach

Matrix-based computation:

* Rows = characters of word 1
* Columns = characters of word 2
* Each cell = minimum edit distance up to that prefix

Time complexity: **O(m Ã— n)** (where m and n are string lengths)

ğŸ“– [Levenshtein Distance](https://en.wikipedia.org/wiki/Levenshtein_distance)


## âš¡ Variants

1. **Hamming Distance**

   * Counts substitutions only
   * Requires strings of equal length
   * Example: *karolin* vs *kathrin* â†’ distance = 3
     ğŸ“– [Hamming Distance](https://en.wikipedia.org/wiki/Hamming_distance)

2. **Damerau-Levenshtein Distance**

   * Includes **transposition** (swap of adjacent letters)
   * Example: *caht* vs *chat* â†’ distance = 1

3. **Weighted Edit Distance**

   * Different costs for different operations
   * Useful in speech recognition, OCR


## ğŸ” Applications

1. **Spell Checking**

   * Find dictionary word with minimum edit distance to input
   * Example: *recieve â†’ receive*

2. **Information Retrieval**

   * Match queries with spelling variations

3. **Plagiarism Detection**

   * Compare similarity of documents

4. **Computational Biology**

   * DNA sequence alignment (edit distance between gene sequences)

ğŸ“– [Applications of Edit Distance](https://en.wikipedia.org/wiki/Edit_distance#Applications)


## ğŸ§  Example Task

Query: *intension*
Candidate dictionary words:

* *intention* (distance 1)
* *in tension* (distance 2)

Likely correction: **intention**


## ğŸ“Œ Summary

* **Edit distance** is a fundamental similarity metric in NLP.
* Variants (Hamming, Damerau-Levenshtein, weighted) adapt it to different use cases.
* Widely applied in **spell checking, search engines, plagiarism detection, and bioinformatics**.


## References

* [Edit Distance](https://en.wikipedia.org/wiki/Edit_distance)
* [Levenshtein Distance](https://en.wikipedia.org/wiki/Levenshtein_distance)
* [Hamming Distance](https://en.wikipedia.org/wiki/Hamming_distance)
* [Damerauâ€“Levenshtein Distance](https://en.wikipedia.org/wiki/Damerau%E2%80%93Levenshtein_distance)
* [Applications of Edit Distance](https://en.wikipedia.org/wiki/Edit_distance#Applications)


# Natural Language Processing â€“ Lecture 8

## ğŸ“Š N-Gram Language Models

This lecture introduces **n-gram models**, a fundamental approach for **language modeling** and **probabilistic text analysis**.


## ğŸ§  What is a Language Model?

* A **language model (LM)** assigns a **probability** to a sequence of words.
* Example:

  * $P(\text{â€œI am a studentâ€})$ > $P(\text{â€œStudent a am Iâ€})$

ğŸ“– [Language Model (Wikipedia)](https://en.wikipedia.org/wiki/Language_model)


## ğŸ”¢ The Chain Rule of Probability

For a sequence of words $w_1, w_2, ..., w_n$:

$$
P(w_1, w_2, ..., w_n) = P(w_1) \cdot P(w_2|w_1) \cdot P(w_3|w_1, w_2) \cdot ... \cdot P(w_n|w_1, ..., w_{n-1})
$$

* But exact computation is infeasible due to **data sparsity**.


## âœ‚ï¸ Markov Assumption

* Approximate by considering only **last few words**:

$$
  P(w_n | w_1, w_2, ..., w_{n-1}) \approx P(w_n | w_{n-(n-1)}, ..., w_{n-1})
$$



## ğŸ“Œ N-Gram Models

### 1. **Unigram Model**

* Assumes independence:

$$
  P(w_1, w_2, ..., w_n) = \prod_i P(w_i)
$$

### 2. **Bigram Model**

* Considers pairs of words:

$$
  P(w_1, w_2, ..., w_n) \approx \prod_i P(w_i | w_{i-1})
$$

### 3. **Trigram Model**

* Considers triples of words:

$$
  P(w_1, w_2, ..., w_n) \approx \prod_i P(w_i | w_{i-2}, w_{i-1})
$$

ğŸ“– [N-Gram Model](https://en.wikipedia.org/wiki/N-gram)



## ğŸ“Š Estimation from Corpus

* **Maximum Likelihood Estimation (MLE):**

  * Bigram probability:

$$
    P(w_i | w_{i-1}) = \frac{C(w_{i-1}, w_i)}{C(w_{i-1})}
$$

  where:

  * $C(x)$ = count of occurrence in corpus



## âš ï¸ Data Sparsity Problem

* Many valid word sequences may **never appear** in training data.
* Example:

  * "He likes mango juice" may not occur in corpus, but is valid.



## ğŸ§® Smoothing Techniques

1. **Add-One (Laplace) Smoothing**

   * Add 1 to all counts.
   * Problem: Overestimates rare events.

2. **Add-k Smoothing**

   * Add small $k$ instead of 1.

3. **Good-Turing Smoothing**

   * Re-estimates probabilities of **unseen events**.

4. **Backoff and Interpolation**

   * Use higher-order n-gram if available, otherwise backoff to lower-order.

ğŸ“– [Smoothing (NLP)](https://en.wikipedia.org/wiki/Additive_smoothing)


## ğŸ” Applications of N-Gram Models

* **Spell correction**
* **Query completion** in search engines
* **Speech recognition**
* **Text generation**



## ğŸ“Œ Summary

* **N-gram models** approximate language probability using limited context.
* **Unigram, bigram, trigram** models balance complexity vs accuracy.
* **Smoothing** addresses the problem of unseen words.
* They form the basis for many NLP tasks, though modern deep learning models (e.g., [RNNs](https://en.wikipedia.org/wiki/Recurrent_neural_network), [Transformers](https://en.wikipedia.org/wiki/Transformer_%28machine_learning_model%29)) now dominate.



## References

* [Language Model](https://en.wikipedia.org/wiki/Language_model)
* [N-Gram](https://en.wikipedia.org/wiki/N-gram)
* [Additive Smoothing](https://en.wikipedia.org/wiki/Additive_smoothing)
* [Good-Turing Estimation](https://en.wikipedia.org/wiki/Good%E2%80%93Turing_frequency_estimation)
* [Recurrent Neural Network](https://en.wikipedia.org/wiki/Recurrent_neural_network)
* [Transformer Model](https://en.wikipedia.org/wiki/Transformer_%28machine_learning_model%29)


# Natural Language Processing â€“ Lecture 9

## ğŸ§® Evaluation of Language Models

This lecture covers **how to evaluate n-gram language models**, focusing on metrics like **perplexity**, and explores their applications.



## ğŸ¯ Why Evaluate?

* To check **how well a language model predicts unseen text**.
* Evaluation helps in:

  * Comparing models (bigram vs trigram, etc.)
  * Measuring **generalization** beyond training data



## ğŸ“Š Evaluation Metrics

### 1. **Likelihood**

* Measure: Probability assigned to a test corpus
* Problem: Direct probabilities are extremely small (due to long sequences)



### 2. **Cross-Entropy**

$$
H = -\frac{1}{N} \sum_{i=1}^{N} \log_2 P(w_i | context)
$$

* $N$ = number of words
* Lower $H$ means better prediction

ğŸ“– [Cross-Entropy (Wikipedia)](https://en.wikipedia.org/wiki/Cross_entropy)



### 3. **Perplexity**

* Standard metric for language models:

$$
PP(W) = 2^H = P(w_1, w_2, ..., w_N)^{-\frac{1}{N}}
$$

* Interpretation: **Average branching factor** of the model
* Lower perplexity = better model

ğŸ“– [Perplexity (Wikipedia)](https://en.wikipedia.org/wiki/Perplexity)



## âš¡ Practical Notes on Perplexity

* Sensitive to **training/test mismatch**
* Works best when:

  * Training and test corpora are from the **same domain**
* Example:

  * A news-trained model may fail on conversational data



## ğŸ§© Applications of N-Gram Language Models

1. **Spell Correction**

   * Rank candidate words by probability

2. **Autocomplete / Query Completion**

   * Predict next word in search engines

3. **Speech Recognition**

   * Disambiguate between homophones using context

4. **Machine Translation**

   * Ensure fluency of translated sentences

ğŸ“– [Statistical Language Modeling Applications](https://en.wikipedia.org/wiki/Language_model#Applications)



## ğŸ“Œ Example

Sentence: *â€œI want to eat â€¦â€*

* Bigram model:

  * $P(\text{â€œfoodâ€}|\text{eat})$ vs $P(\text{â€œsleepâ€}|\text{eat})$
  * Likely: "food"



## ğŸ“Œ Summary

* Evaluation of language models is essential to measure performance.
* **Perplexity** is the most widely used metric.
* Lower perplexity = better predictive power.
* Applications: **spell correction, autocomplete, speech recognition, translation**.



## References

* [Cross-Entropy](https://en.wikipedia.org/wiki/Cross_entropy)
* [Perplexity](https://en.wikipedia.org/wiki/Perplexity)
* [Language Model Applications](https://en.wikipedia.org/wiki/Language_model#Applications)


# Natural Language Processing â€“ Lecture 10

## ğŸ“Š Part-of-Speech (POS) Tagging â€“ Introduction

This lecture introduces **Part-of-Speech (POS) tagging**, a key step in NLP for assigning **grammatical categories** (e.g., noun, verb, adjective) to words.


## ğŸ§  What is POS Tagging?

* **Definition:** Assigning a syntactic category (POS tag) to each word in a sentence.
* Example:

  * *â€œTime flies like an arrowâ€*

    * Time â†’ Noun
    * flies â†’ Verb
    * like â†’ Preposition
    * an â†’ Article
    * arrow â†’ Noun

ğŸ“– [POS Tagging (Wikipedia)](https://en.wikipedia.org/wiki/Part-of-speech_tagging)


## ğŸ“š POS Tagsets

* **Penn Treebank Tagset** (most widely used in English NLP)

  * NN = noun, VB = verb, JJ = adjective, RB = adverb
* Other tagsets:

  * Brown Corpus Tagset
  * Universal POS Tagset (cross-lingual)

ğŸ“– [Penn Treebank](https://en.wikipedia.org/wiki/Penn_Treebank)


## ğŸ§© Ambiguity in POS Tagging

* Words can have **multiple POS tags** depending on context.
* Example:

  * *â€œBook a flightâ€* â†’ book = Verb
  * *â€œRead a bookâ€* â†’ book = Noun

This is a core challenge in tagging.


## ğŸ”¢ Approaches to POS Tagging

### 1. **Rule-Based Tagging**

* Handcrafted rules based on grammar + context
* Example: â€œif a word ends in *-ing*, itâ€™s probably a verbâ€
* Limitation: Requires **linguistic expertise**

ğŸ“– [Rule-Based Tagging](https://en.wikipedia.org/wiki/Part-of-speech_tagging#Rule-based_tagging)


### 2. **Statistical Tagging**

* Uses probability models (trained from annotated corpora).
* Example methods:

  * **Hidden Markov Models (HMMs)**
  * **N-gram models** for tag sequences
* Finds the **most likely sequence of tags** given the sentence.

ğŸ“– [Hidden Markov Model](https://en.wikipedia.org/wiki/Hidden_Markov_model)


### 3. **Transformation-Based Tagging (TBL)**

* Introduced by Eric Brill (called **Brill Tagger**).
* Learns transformation rules from a tagged corpus.
* Hybrid of rule-based and statistical.

ğŸ“– [Brill Tagger](https://en.wikipedia.org/wiki/Brill_tagger)


### 4. **Neural Tagging**

* Modern approach: Deep learning models

  * **RNNs, LSTMs, Transformers** ([BERT](https://en.wikipedia.org/wiki/BERT_%28language_model%29))
* Use **contextual embeddings** to resolve ambiguities.
* Achieve state-of-the-art accuracy.


## ğŸ” Applications of POS Tagging

* **Information extraction**
* **Named Entity Recognition (NER)**
* **Parsing**
* **Machine translation**
* **Speech recognition**


## ğŸ“Œ Example Sentence

Sentence: *â€œCan you can a can as a canner can can a can?â€*

* can â†’ modal verb
* can â†’ main verb (preserve food)
* can â†’ noun (container)

Demonstrates **POS ambiguity** and importance of context.


## ğŸ“Œ Summary

* POS tagging assigns **syntactic roles** to words.
* Challenges: **ambiguity, context-dependence**.
* Approaches:

  * **Rule-based**
  * **Statistical (HMM, n-grams)**
  * **Transformation-based**
  * **Neural (modern SOTA)**
* Essential for higher-level NLP tasks.


## References

* [POS Tagging](https://en.wikipedia.org/wiki/Part-of-speech_tagging)
* [Penn Treebank](https://en.wikipedia.org/wiki/Penn_Treebank)
* [Hidden Markov Models](https://en.wikipedia.org/wiki/Hidden_Markov_model)
* [Brill Tagger](https://en.wikipedia.org/wiki/Brill_tagger)
* [BERT](https://en.wikipedia.org/wiki/BERT_%28language_model%29)


# Natural Language Processing â€“ Lecture 11

## ğŸ” Part-of-Speech (POS) Tagging â€“ Hidden Markov Models

This lecture explains how **Hidden Markov Models (HMMs)** can be applied to **POS tagging**, building on the statistical approach introduced earlier.


## ğŸ§  Recap: POS Tagging Problem

* Input: Sentence = sequence of words
* Output: Sequence of **POS tags** (one per word)
* Challenge: Words can belong to **multiple categories** depending on context

  * Example: *book* â†’ Noun (*â€œRead a bookâ€*) vs Verb (*â€œBook a flightâ€*)

ğŸ“– [POS Tagging (Wikipedia)](https://en.wikipedia.org/wiki/Part-of-speech_tagging)


## ğŸ² Hidden Markov Models (HMMs)

### Key Idea

* Model tagging as a **sequence prediction problem**.
* Assume:

  * **Tags** are hidden states
  * **Words** are observed emissions

ğŸ“– [Hidden Markov Model (Wikipedia)](https://en.wikipedia.org/wiki/Hidden_Markov_model)


### HMM Components

1. **States** â†’ POS tags (e.g., NN, VB, JJ)
2. **Observations** â†’ Words in the sentence
3. **Transition Probabilities**

   * $P(t_i | t_{i-1})$ â†’ probability of a tag given the previous tag
4. **Emission Probabilities**

   * $P(w_i | t_i)$ â†’ probability of a word given its tag


## ğŸ”¢ The Tagging Task

Goal:
Find the **most likely sequence of tags** $T = t_1, t_2, â€¦, t_n$ for given words $W = w_1, w_2, â€¦, w_n$.

Formally:

$$
\hat{T} = \arg\max_T P(T | W)
$$

Using Bayesâ€™ Rule:

$$
P(T | W) \propto P(W | T) \cdot P(T)
$$

* $P(T)$ â†’ Transition probabilities
* $P(W|T)$ â†’ Emission probabilities


## ğŸ§® Example

Sentence: *â€œFish swimâ€*

* Possible tags:

  * *Fish*: Noun or Verb
  * *Swim*: Verb or Noun

HMM resolves ambiguity by maximizing joint probability of tags + words.


## ğŸ›  The Viterbi Algorithm

* **Dynamic programming algorithm** for finding the best tag sequence.
* Efficiently computes the **most likely path** through HMM states.
* Time complexity: $O(N \cdot T^2)$

  * $N$ = sentence length
  * $T$ = number of tags

ğŸ“– [Viterbi Algorithm (Wikipedia)](https://en.wikipedia.org/wiki/Viterbi_algorithm)


## âš ï¸ Challenges with HMM POS Tagging

1. **Data sparsity** â€“ Rare transitions or emissions may have zero probability

   * Solution: **Smoothing** techniques
2. **Unknown words (OOV problem)**

   * Handle with morphological rules or character-level models
3. **Independence assumptions** (Markov, emission) are often too simplistic


## ğŸ“Š Modern Alternatives

* **Conditional Random Fields (CRFs)**
* **Neural models** (RNNs, LSTMs, Transformers like [BERT](https://en.wikipedia.org/wiki/BERT_%28language_model%29))
* Outperform HMMs by capturing **longer dependencies** and **rich features**

ğŸ“– [Conditional Random Fields](https://en.wikipedia.org/wiki/Conditional_random_field)


## ğŸ“Œ Summary

* POS tagging with HMM:

  * Tags = hidden states
  * Words = emissions
  * Transition + emission probabilities define model
* **Viterbi algorithm** used to find best tag sequence
* Limitations: data sparsity, OOV handling, independence assumptions
* Modern approaches: **CRFs, neural networks** outperform HMMs


## References

* [POS Tagging](https://en.wikipedia.org/wiki/Part-of-speech_tagging)
* [Hidden Markov Model](https://en.wikipedia.org/wiki/Hidden_Markov_model)
* [Viterbi Algorithm](https://en.wikipedia.org/wiki/Viterbi_algorithm)
* [Conditional Random Field](https://en.wikipedia.org/wiki/Conditional_random_field)
* [BERT](https://en.wikipedia.org/wiki/BERT_%28language_model%29)


# Natural Language Processing â€“ Lecture 13

## ğŸŒ³ Introduction to Syntax and Parsing

This lecture introduces **syntax** in NLP, focusing on how sentences are structured and how **parsers** analyze grammatical relations between words.



## ğŸ§  What is Syntax?

* **Syntax** = Study of **sentence structure** and how words combine to form phrases/clauses.
* Important for:

  * Understanding **grammatical relationships**
  * Building higher-level NLP applications (translation, QA, etc.)

ğŸ“– [Syntax (Wikipedia)](https://en.wikipedia.org/wiki/Syntax)


## ğŸ“– Phrase Structure in Language

* Sentences are not just sequences of words â€” they have **hierarchical structure**.
* Example:

  * Sentence: *â€œThe boy saw the man with a telescope.â€*
  * Ambiguity:

    * Did the **boy** have the telescope?
    * Or the **man**?

ğŸ“– [Parse Trees](https://en.wikipedia.org/wiki/Parse_tree)


## ğŸ§© Grammar Formalisms

### 1. **Context-Free Grammar (CFG)**

* A grammar consists of:

  * **Non-terminals** (syntactic categories like NP = Noun Phrase, VP = Verb Phrase)
  * **Terminals** (actual words)
  * **Production rules**
  * **Start symbol** (usually S = Sentence)

* Example rules:

  * $S \to NP \; VP$
  * $NP \to Det \; N$
  * $VP \to V \; NP$

ğŸ“– [Context-Free Grammar](https://en.wikipedia.org/wiki/Context-free_grammar)



### 2. **Parse Trees**

* Show **hierarchical structure** of a sentence.
* Example:

  * *â€œThe cat sat on the matâ€*
  * Tree structure:

    * S â†’ NP VP
    * NP â†’ Det N
    * VP â†’ V PP


## âš™ï¸ Parsing

### Definition

* **Parsing** = Process of analyzing a sentence according to a grammar to produce its structure.

ğŸ“– [Parsing (Wikipedia)](https://en.wikipedia.org/wiki/Parsing)



### Types of Parsers

1. **Top-Down Parsing**

   * Start from root (S) and expand until terminals match input words
   * May generate many invalid parses

2. **Bottom-Up Parsing**

   * Start from words and combine upwards until reaching root
   * May attempt invalid combinations

3. **Chart Parsing**

   * Dynamic programming approach
   * Avoids recomputation of substructures

ğŸ“– [Chart Parser](https://en.wikipedia.org/wiki/Chart_parser)


## âš ï¸ Parsing Ambiguities

* Multiple valid parses are often possible.
* Example: *â€œI saw the man with the telescope.â€*

  * Two possible parse trees
* Leads to **syntactic ambiguity** problem.

ğŸ“– [Syntactic Ambiguity](https://en.wikipedia.org/wiki/Syntactic_ambiguity)



## ğŸ“Š Applications of Parsing

* **Machine Translation** (syntactic alignment between languages)
* **Information Extraction** (finding subjectâ€“verbâ€“object relations)
* **Question Answering** (interpreting query structure)
* **Dialogue Systems**


## ğŸ“Œ Summary

* **Syntax** studies how words form grammatical structures.
* **CFGs** and **parse trees** model hierarchical structure.
* **Parsing algorithms** (top-down, bottom-up, chart) build syntactic trees.
* Parsing faces **ambiguity** but is critical for high-level NLP tasks.



## References

* [Syntax](https://en.wikipedia.org/wiki/Syntax)
* [Parse Tree](https://en.wikipedia.org/wiki/Parse_tree)
* [Context-Free Grammar](https://en.wikipedia.org/wiki/Context-free_grammar)
* [Parsing](https://en.wikipedia.org/wiki/Parsing)
* [Chart Parser](https://en.wikipedia.org/wiki/Chart_parser)
* [Syntactic Ambiguity](https://en.wikipedia.org/wiki/Syntactic_ambiguity)


# Natural Language Processing â€“ Lecture 14

## ğŸŒ³ Parsing Algorithms

This lecture continues the study of **syntax and parsing**, focusing on **algorithms** for building parse trees from context-free grammars (CFGs).



## âš™ï¸ Parsing as a Computational Problem

* Input: Sentence = sequence of words
* Output: **Parse tree** showing syntactic structure
* Task: Decide whether the sentence belongs to the grammar and, if yes, generate its structure

ğŸ“– [Parsing (Wikipedia)](https://en.wikipedia.org/wiki/Parsing)


## ğŸ”¼ Top-Down Parsing

### Approach

* Start from **start symbol (S)**
* Expand using grammar rules until terminals match input

### Example

* Grammar:

  * $S \to NP \; VP$
  * $NP \to Det \; N$
  * $VP \to V \; NP$
* Input: *â€œThe cat chased the dogâ€*

### Pros

* Easy to implement
* Explores possible parses systematically

### Cons

* May generate many **invalid parses**
* Redundant work (recomputing same subtrees)

ğŸ“– [Top-Down Parsing](https://en.wikipedia.org/wiki/Top-down_parsing)



## ğŸ”½ Bottom-Up Parsing

### Approach

* Start from **words (terminals)**
* Combine into larger constituents until reaching S

### Example

* Input: *â€œThe cat chased the dogâ€*
* Begin with words â†’ form NP and VP â†’ assemble into S

### Pros

* Efficient when valid parses exist

### Cons

* May build many **incorrect structures**

ğŸ“– [Bottom-Up Parsing](https://en.wikipedia.org/wiki/Bottom-up_parsing)



## ğŸ“Š Chart Parsing (Dynamic Programming)

* **Dynamic programming** method that avoids redundant recomputation
* Stores intermediate results in a **chart** (table)

### Example Algorithm: **CYK Parser**

* Works with **CFGs in Chomsky Normal Form (CNF)**
* Time complexity: $O(n^3)$ for sentence length $n$
* Useful for theoretical and practical parsing tasks

ğŸ“– [CYK Algorithm](https://en.wikipedia.org/wiki/CYK_algorithm)



## âš ï¸ Parsing Ambiguity

* Many sentences allow **multiple parse trees**
* Example: *â€œI saw the man with the telescopeâ€*

  * Ambiguity: Who had the telescope?

### Handling Ambiguity

1. Generate **all possible parses**
2. Rank parses using **probabilistic models** (e.g., [Probabilistic CFG](https://en.wikipedia.org/wiki/Stochastic_context-free_grammar))



## ğŸ§© Applications of Parsing

* **Machine Translation** (syntactic alignment across languages)
* **Information Extraction** (subjectâ€“verbâ€“object relations)
* **Text Summarization** (understanding sentence structure)
* **Dialogue Systems**



## ğŸ“Œ Summary

* Parsing algorithms analyze sentence structure using CFGs.
* **Top-down** and **bottom-up** methods are simple but inefficient.
* **Chart parsing** (CYK) reduces redundant work via dynamic programming.
* Ambiguity remains a challenge â†’ handled using **probabilistic parsing**.



## References

* [Parsing](https://en.wikipedia.org/wiki/Parsing)
* [Top-Down Parsing](https://en.wikipedia.org/wiki/Top-down_parsing)
* [Bottom-Up Parsing](https://en.wikipedia.org/wiki/Bottom-up_parsing)
* [CYK Algorithm](https://en.wikipedia.org/wiki/CYK_algorithm)
* [Probabilistic CFG](https://en.wikipedia.org/wiki/Stochastic_context-free_grammar)


# Natural Language Processing â€“ Lecture 15

## ğŸ“Š Probabilistic Parsing

This lecture introduces **probabilistic approaches to parsing**, where parse trees are ranked using probabilities instead of treating all parses equally.


## âš ï¸ Why Probabilistic Parsing?

* **Problem:** Many sentences are **syntactically ambiguous**.

  * Example: *â€œI saw the man with the telescope.â€*
  * Ambiguity: Who had the telescope?
* **Solution:** Assign probabilities to parses â†’ choose the most likely one.

ğŸ“– [Syntactic Ambiguity](https://en.wikipedia.org/wiki/Syntactic_ambiguity)


## ğŸ§  Probabilistic Context-Free Grammars (PCFGs)

### Definition

* A **Context-Free Grammar (CFG)** where each production rule has a **probability**.
* Example:

  * $S \to NP \; VP$ $P = 1.0$
  * $NP \to Det \; N$ $P = 0.6$
  * $NP \to NP \; PP$ $P = 0.4$

ğŸ“– [Stochastic Context-Free Grammar](https://en.wikipedia.org/wiki/Stochastic_context-free_grammar)



### Parse Probability

* The probability of a parse tree is the **product of the rule probabilities** used.

$$
P(\text{Parse}) = \prod_i P(\text{Rule}_i)
$$

* Select parse with **maximum probability**.



## ğŸ§® Example

Sentence: *â€œThe cat saw the dog with a telescopeâ€*

* Parse 1: *dog has telescope*
* Parse 2: *cat used telescope*
* PCFG assigns probabilities â†’ more likely parse chosen.


## âš™ï¸ Estimating Probabilities

1. **From a Treebank**

   * Example: [Penn Treebank](https://en.wikipedia.org/wiki/Penn_Treebank)
   * Count frequency of rule usage in annotated corpus.
   * Rule probability:

$$
     P(A \to \beta) = \frac{\text{Count}(A \to \beta)}{\text{Count}(A)}
$$

2. **Smoothing**

   * Handle rare/unseen rules
   * Techniques: Add-k, Good-Turing, backoff


## ğŸ“Š Parsing with PCFGs

* Algorithms:

  * **Probabilistic CYK Parsing**
  * **Probabilistic Chart Parsing**
* Select the **most probable parse tree**.

ğŸ“– [CYK Algorithm](https://en.wikipedia.org/wiki/CYK_algorithm)


## âš ï¸ Limitations of PCFGs

1. **Context Independence**

   * Assumes independence of rules (unrealistic).
2. **Preference Bias**

   * Overgenerates shallow parses (shorter trees often get higher probability).
3. **Lexical Information Missing**

   * Doesnâ€™t incorporate word-level preferences.


## ğŸ” Improvements Beyond PCFG

* **Lexicalized PCFGs** â†’ include head words in rules
* **Dependency Parsing** â†’ focus on word-to-word relations
* **Neural Parsers** â†’ use embeddings & deep models (e.g., [Transformers](https://en.wikipedia.org/wiki/Transformer_%28machine_learning_model%29))


## ğŸ“Œ Summary

* **PCFGs** assign probabilities to grammar rules â†’ rank parses.
* Parse probability = product of rule probabilities.
* Trained from **treebanks** like Penn Treebank.
* PCFGs improve parsing but have limitations â†’ addressed by lexicalized and neural models.


## References

* [Syntactic Ambiguity](https://en.wikipedia.org/wiki/Syntactic_ambiguity)
* [Stochastic Context-Free Grammar](https://en.wikipedia.org/wiki/Stochastic_context-free_grammar)
* [Penn Treebank](https://en.wikipedia.org/wiki/Penn_Treebank)
* [CYK Algorithm](https://en.wikipedia.org/wiki/CYK_algorithm)
* [Transformer Model](https://en.wikipedia.org/wiki/Transformer_%28machine_learning_model%29)



# Natural Language Processing â€“ Lecture 16

## ğŸ”— Dependency Parsing

This lecture introduces **dependency parsing**, an alternative to phrase-structure parsing, where syntax is represented as **relations between words** rather than hierarchical constituents.


## ğŸ§  What is Dependency Parsing?

* Focuses on **binary relations** between words: **head â†’ dependent**.
* Each word (except the root) depends on another word.
* Produces a **dependency tree** instead of a phrase-structure tree.

ğŸ“– [Dependency Grammar](https://en.wikipedia.org/wiki/Dependency_grammar)


## ğŸ“Š Example

Sentence: *â€œThe cat chased the mouseâ€*

* Dependencies:

  * chased (ROOT)
  * cat â†’ subject of chased
  * mouse â†’ object of chased
  * the â†’ determiner of cat
  * the â†’ determiner of mouse

Result: **Head-dependent tree** showing grammatical functions.


## ğŸ“š Dependency Relations

Common dependency labels (from [Universal Dependencies](https://universaldependencies.org/)):

* **nsubj** â†’ nominal subject
* **dobj** â†’ direct object
* **det** â†’ determiner
* **amod** â†’ adjectival modifier
* **prep** â†’ prepositional modifier

Example: *â€œShe ate the red apple.â€*

* nsubj(she, ate)
* dobj(apple, ate)
* det(the, apple)
* amod(red, apple)


## âš™ï¸ Dependency Parsing vs Phrase-Structure Parsing

| Aspect         | Phrase-Structure Parsing           | Dependency Parsing                          |
| -------------- | ---------------------------------- | ------------------------------------------- |
| Representation | Constituents (NP, VP, etc.)        | Word-to-word dependencies                   |
| Tree           | Hierarchical, nested phrases       | Flat headâ€“dependent arcs                    |
| Useful for     | Syntax analysis, linguistic theory | Information extraction, relation extraction |

ğŸ“– [Constituency vs Dependency](https://en.wikipedia.org/wiki/Dependency_grammar#Comparison_with_constituency_grammar)


## ğŸ›  Dependency Parsing Algorithms

### 1. **Transition-Based Parsing**

* Incrementally build parse tree using actions:

  * SHIFT, REDUCE, LEFT-ARC, RIGHT-ARC
* Example: **Arc-Standard Algorithm**
* Efficient: Linear-time parsing ($O(n)$)

ğŸ“– [Transition-Based Parsing](https://en.wikipedia.org/wiki/Dependency_grammar#Transition-based_parsing)


### 2. **Graph-Based Parsing**

* Treat parsing as finding **maximum spanning tree** over words.
* Use global optimization over possible parses.
* Typically slower ($O(n^2)$ or $O(n^3)$).

ğŸ“– [Graph-Based Dependency Parsing](https://en.wikipedia.org/wiki/Dependency_grammar#Graph-based_parsing)


## âš ï¸ Challenges in Dependency Parsing

* **Ambiguity**: Multiple valid parses possible.
* **Non-projective dependencies** (crossing arcs) â†’ common in free word-order languages like Czech, Russian.
* Requires **large annotated corpora** (e.g., Universal Dependencies).

ğŸ“– [Non-projective Parsing](https://en.wikipedia.org/wiki/Dependency_grammar#Projectivity)


## ğŸ” Applications of Dependency Parsing

* **Information Extraction** â†’ subjectâ€“verbâ€“object relations
* **Relation Extraction** â†’ who did what to whom
* **Question Answering** â†’ syntactic dependencies help identify answers
* **Machine Translation** â†’ syntax-informed alignments



## ğŸ“Œ Summary

* Dependency parsing represents syntax as **head-dependent relations**.
* Two main approaches: **transition-based** (efficient) and **graph-based** (global).
* Challenges: ambiguity, non-projective structures, domain adaptation.
* Widely applied in **IE, QA, MT, semantic parsing**.


## References

* [Dependency Grammar](https://en.wikipedia.org/wiki/Dependency_grammar)
* [Universal Dependencies](https://universaldependencies.org/)
* [Transition-Based Parsing](https://en.wikipedia.org/wiki/Dependency_grammar#Transition-based_parsing)
* [Graph-Based Parsing](https://en.wikipedia.org/wiki/Dependency_grammar#Graph-based_parsing)
* [Projectivity in Parsing](https://en.wikipedia.org/wiki/Dependency_grammar#Projectivity)


# Natural Language Processing â€“ Lecture 17

## ğŸ§  Semantics in NLP â€“ Introduction

This lecture introduces **semantics**, the study of **meaning in language**, and its role in NLP systems.


## ğŸ“– What is Semantics?

* **Semantics** = study of **word, phrase, and sentence meaning**.
* Goes beyond syntax (structure) to capture **interpretation**.
* Essential for tasks like **translation, question answering, dialogue, and summarization**.

ğŸ“– [Semantics (Wikipedia)](https://en.wikipedia.org/wiki/Semantics)


## ğŸ”‘ Levels of Meaning

1. **Word-Level Semantics**

   * Meaning of individual words
   * Example: "bank" â†’ financial institution vs riverbank

2. **Sentence-Level Semantics**

   * Compositional meaning from structure + words
   * Example: *â€œJohn loves Maryâ€* vs *â€œMary loves Johnâ€*

3. **Discourse-Level Semantics**

   * Meaning across multiple sentences
   * Example: Pronoun resolution (*â€œJohn went home. He slept.â€*)

ğŸ“– [Lexical Semantics](https://en.wikipedia.org/wiki/Lexical_semantics)


## âš ï¸ Challenges in Semantics

1. **Ambiguity**

   * Lexical: "bark" â†’ tree bark vs dog sound
   * Structural: *â€œold men and womenâ€*

2. **Polysemy vs Homonymy**

   * Polysemy: word with related senses (*paper = material, publication*)
   * Homonymy: unrelated senses (*bat = animal, cricket bat*)

ğŸ“– [Polysemy](https://en.wikipedia.org/wiki/Polysemy) | [Homonymy](https://en.wikipedia.org/wiki/Homonymy)


## ğŸ§® Representing Meaning

### 1. **First-Order Logic (FOL)**

* Expresses meaning formally with **predicates, variables, quantifiers**.
* Example:

  * Sentence: *â€œEvery student passed the examâ€*
  * Representation: $\forall x \, [Student(x) \rightarrow Passed(x, exam)]$

ğŸ“– [First-Order Logic](https://en.wikipedia.org/wiki/First-order_logic)


### 2. **Semantic Roles**

* Assigns roles in events (who did what to whom).
* Example: *â€œMary gave John a bookâ€*

  * Agent = Mary
  * Recipient = John
  * Theme = Book

ğŸ“– [Semantic Role Labeling](https://en.wikipedia.org/wiki/Semantic_role_labeling)


### 3. **Distributional Semantics**

* Word meaning derived from **context of use**.
* Based on the **distributional hypothesis**:

  * *â€œWords that occur in similar contexts have similar meaningsâ€*.
* Example: "dog" and "cat" occur in similar contexts â†’ semantically close.

ğŸ“– [Distributional Semantics](https://en.wikipedia.org/wiki/Distributional_semantics)


## ğŸ§© Lexical Resources

* **WordNet** â†’ semantic network of words (synonyms, hypernyms, hyponyms).
* Example: "car" â†’ synonym set includes "automobile".

ğŸ“– [WordNet](https://en.wikipedia.org/wiki/WordNet)



## ğŸ” Applications of Semantics

* **Word Sense Disambiguation (WSD)**
* **Information Retrieval (IR)** â†’ semantic search
* **Machine Translation**
* **Question Answering**
* **Text Summarization**


## ğŸ“Œ Summary

* Semantics = study of **meaning in language**.
* Levels: word, sentence, discourse.
* Ambiguities (lexical, structural) complicate understanding.
* Representations: **FOL, semantic roles, distributional semantics**.
* Lexical resources like **WordNet** support semantic tasks.


## References

* [Semantics](https://en.wikipedia.org/wiki/Semantics)
* [Lexical Semantics](https://en.wikipedia.org/wiki/Lexical_semantics)
* [Polysemy](https://en.wikipedia.org/wiki/Polysemy)
* [Homonymy](https://en.wikipedia.org/wiki/Homonymy)
* [First-Order Logic](https://en.wikipedia.org/wiki/First-order_logic)
* [Semantic Role Labeling](https://en.wikipedia.org/wiki/Semantic_role_labeling)
* [Distributional Semantics](https://en.wikipedia.org/wiki/Distributional_semantics)
* [WordNet](https://en.wikipedia.org/wiki/WordNet)


# Natural Language Processing â€“ Lecture 18

## ğŸ§  Word Sense Disambiguation (WSD)

This lecture explores **Word Sense Disambiguation (WSD)**, a core task in semantics that deals with resolving **ambiguity in word meanings**.



## ğŸ“– What is WSD?

* **Definition:** Task of determining which sense of a word is used in a given context.
* Example:

  * *â€œHe deposited money in the bank.â€* â†’ bank = financial institution
  * *â€œHe sat by the bank of the river.â€* â†’ bank = riverside

ğŸ“– [Word Sense Disambiguation (Wikipedia)](https://en.wikipedia.org/wiki/Word-sense_disambiguation)



## âš ï¸ Why is WSD Hard?

1. **Polysemy** â†’ One word with multiple related senses (*paper = material, article*).
2. **Homonymy** â†’ Word with unrelated senses (*bat = animal, cricket bat*).
3. **Context Dependence** â†’ Meaning depends on surrounding words.
4. **Granularity** â†’ Fine-grained distinctions in resources like WordNet.

ğŸ“– [Polysemy](https://en.wikipedia.org/wiki/Polysemy) | [Homonymy](https://en.wikipedia.org/wiki/Homonymy)



## ğŸ›  Approaches to WSD

### 1. **Knowledge-Based Methods**

* Use **dictionaries, thesauri, lexical resources**.
* Example: **Lesk Algorithm**

  * Sense disambiguation by **overlap of dictionary definitions**.

ğŸ“– [Lesk Algorithm](https://en.wikipedia.org/wiki/Lesk_algorithm)



### 2. **Supervised Learning**

* Treat WSD as a **classification problem**.
* Input: word in context
* Output: correct sense label
* Requires **annotated corpora** (e.g., SemCor).

ğŸ“– [Supervised WSD](https://en.wikipedia.org/wiki/Word-sense_disambiguation#Supervised_methods)



### 3. **Unsupervised Learning**

* Cluster word occurrences into groups â†’ assume each cluster = sense.
* No annotated data required.
* Limitation: clusters may not map cleanly to dictionary senses.

ğŸ“– [Unsupervised Learning](https://en.wikipedia.org/wiki/Unsupervised_learning)



### 4. **Neural Approaches**

* Use **word embeddings** + **contextual embeddings (BERT, ELMo, GPT)**.
* Context captures sense directly.
* Example: BERT can distinguish *â€œbank (money)â€* vs *â€œbank (river)â€*.

ğŸ“– [BERT](https://en.wikipedia.org/wiki/BERT_%28language_model%29)


## ğŸ“Š Evaluation of WSD

* **Gold-standard datasets**: SemCor, Senseval, SemEval.
* Metrics: **Precision, Recall, F1**.
* Example: Accuracy = % of correctly disambiguated words.

ğŸ“– [Senseval / SemEval](https://en.wikipedia.org/wiki/SemEval)



## ğŸ” Applications of WSD

* **Machine Translation** (choosing correct sense in target language).
* **Information Retrieval** (semantic search).
* **Question Answering** (understanding query intent).
* **Text Summarization** (selecting right meaning for key terms).


## ğŸ“Œ Summary

* **WSD = identifying correct sense of a word in context**.
* Ambiguity arises from **polysemy, homonymy, context dependence**.
* Approaches: **knowledge-based, supervised, unsupervised, neural**.
* Applications: **MT, IR, QA, summarization**.



## References

* [Word Sense Disambiguation](https://en.wikipedia.org/wiki/Word-sense_disambiguation)
* [Polysemy](https://en.wikipedia.org/wiki/Polysemy)
* [Homonymy](https://en.wikipedia.org/wiki/Homonymy)
* [Lesk Algorithm](https://en.wikipedia.org/wiki/Lesk_algorithm)
* [Supervised Methods](https://en.wikipedia.org/wiki/Word-sense_disambiguation#Supervised_methods)
* [Unsupervised Learning](https://en.wikipedia.org/wiki/Unsupervised_learning)
* [BERT](https://en.wikipedia.org/wiki/BERT_%28language_model%29)
* [SemEval](https://en.wikipedia.org/wiki/SemEval)


# Natural Language Processing â€“ Lecture 19

## ğŸ§© Distributional Semantics

This lecture covers **distributional semantics**, an approach to modeling word meaning based on **context of usage** in large corpora.



## ğŸ“– The Distributional Hypothesis

* **Idea:** *â€œWords that occur in similar contexts tend to have similar meanings.â€*
* Example:

  * "dog" and "cat" both occur near *pet, animal, fur, feed* â†’ semantically similar.

ğŸ“– [Distributional Semantics (Wikipedia)](https://en.wikipedia.org/wiki/Distributional_semantics)


## ğŸ“Š Representing Words in Vector Space

### 1. **Co-occurrence Matrices**

* Rows = target words
* Columns = context words
* Entries = frequency (or weighted frequency) of co-occurrence
* Example: â€œdogâ€ appears often with â€œbark, pet, animalâ€

ğŸ“– [Vector Space Model](https://en.wikipedia.org/wiki/Vector_space_model)


### 2. **Dimensionality Reduction**

* Raw co-occurrence vectors are **high-dimensional & sparse**
* Apply techniques like:

  * **Singular Value Decomposition (SVD)**
  * **Latent Semantic Analysis (LSA)**

ğŸ“– [Latent Semantic Analysis](https://en.wikipedia.org/wiki/Latent_semantic_analysis)


## ğŸ›  Word Similarity

* Compute similarity between word vectors using:

  * **Cosine similarity**
  * **Euclidean distance**

Example:

* sim(dog, cat) > sim(dog, car)

ğŸ“– [Cosine Similarity](https://en.wikipedia.org/wiki/Cosine_similarity)


## âš™ï¸ Weighting Schemes

### 1. **TF-IDF (Term Frequencyâ€“Inverse Document Frequency)**

* Reduces weight of frequent but uninformative words.

### 2. **PPMI (Positive Pointwise Mutual Information)**

* Measures association strength between word and context.

ğŸ“– [TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) | [PMI](https://en.wikipedia.org/wiki/Pointwise_mutual_information)



## ğŸŒ From Word Vectors to Word Embeddings

* Distributional semantics â†’ foundation of **neural word embeddings**.
* Word2Vec, GloVe, FastText build on these ideas.

ğŸ“– [Word Embedding](https://en.wikipedia.org/wiki/Word_embedding)



## ğŸ” Applications of Distributional Semantics

* **Synonym detection**
* **Thesaurus construction**
* **Information retrieval** (semantic search)
* **Text classification & clustering**
* **Word embeddings training**



## ğŸ“Œ Summary

* **Distributional hypothesis:** meaning from context.
* Represented using **co-occurrence matrices** and reduced with **SVD/LSA**.
* Word similarity measured via **cosine similarity** or PMI.
* Basis for **modern embeddings** like Word2Vec, GloVe, FastText.



## References

* [Distributional Semantics](https://en.wikipedia.org/wiki/Distributional_semantics)
* [Vector Space Model](https://en.wikipedia.org/wiki/Vector_space_model)
* [Latent Semantic Analysis](https://en.wikipedia.org/wiki/Latent_semantic_analysis)
* [Cosine Similarity](https://en.wikipedia.org/wiki/Cosine_similarity)
* [TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)
* [Pointwise Mutual Information](https://en.wikipedia.org/wiki/Pointwise_mutual_information)
* [Word Embedding](https://en.wikipedia.org/wiki/Word_embedding)

# Natural Language Processing â€“ Lecture 20

## ğŸ”¡ Word Embeddings â€“ Introduction

This lecture introduces **word embeddings**, dense vector representations of words that capture **semantic and syntactic relationships**.


## ğŸ“– What are Word Embeddings?

* A method of representing words as **low-dimensional continuous vectors**.
* Unlike sparse one-hot vectors, embeddings capture **semantic similarity**.
* Example:

  * *vector(dog)* â‰ˆ *vector(cat)* (close in space)
  * *vector(dog)* far from *vector(car)*

ğŸ“– [Word Embedding (Wikipedia)](https://en.wikipedia.org/wiki/Word_embedding)


## ğŸ”¢ From Distributional Semantics to Embeddings

* Based on the **distributional hypothesis**: words in similar contexts have similar meanings.
* Word embeddings are **learned automatically** from large corpora.
* Improve over **co-occurrence matrices + SVD** by using **neural networks**.

ğŸ“– [Distributional Semantics](https://en.wikipedia.org/wiki/Distributional_semantics)


## ğŸ§© Properties of Word Embeddings

1. **Semantic Similarity**

   * Cosine similarity between vectors captures meaning.
   * Example: sim(â€œkingâ€, â€œqueenâ€) > sim(â€œkingâ€, â€œcarâ€).

2. **Analogical Reasoning**

   * Famous property:

$$
     \text{vector(king)} - \text{vector(man)} + \text{vector(woman)} \approx \text{vector(queen)}
$$

3. **Clustering**

   * Similar words form clusters: animals, professions, places.


## ğŸ›  Techniques for Learning Word Embeddings

### 1. **Word2Vec**

* Introduced by Mikolov et al. (2013).
* Two architectures:

  * **CBOW (Continuous Bag-of-Words)** â†’ predict word from context.
  * **Skip-gram** â†’ predict context from word.

ğŸ“– [Word2Vec](https://en.wikipedia.org/wiki/Word2vec)


### 2. **GloVe (Global Vectors)**

* Uses **global co-occurrence statistics**.
* Factorizes wordâ€“context matrix.

ğŸ“– [GloVe](https://en.wikipedia.org/wiki/GloVe_%28machine_learning%29)


### 3. **FastText**

* Extends Word2Vec by using **subword information**.
* Helps handle rare/unknown words.

ğŸ“– [FastText](https://en.wikipedia.org/wiki/FastText)


## ğŸ“Š Comparing Representations

| Representation       | Type   | Dimensionality | Captures Meaning? |   |      |
| -------------------- | ------ | -------------- | ----------------- | - | ---- |
| One-Hot Vector       | Sparse |                | V                 |   | âŒ No |
| Co-occurrence Vector | Sparse | High           | âœ… Partial         |   |      |
| Word Embedding       | Dense  | 100â€“300        | âœ… Yes             |   |      |


## ğŸ” Applications of Word Embeddings

* **Information Retrieval** (semantic search)
* **Text Classification** (sentiment analysis, spam detection)
* **Machine Translation** (cross-lingual embeddings)
* **Question Answering**
* **Recommendation Systems**


## ğŸ“Œ Summary

* Word embeddings = **dense, low-dimensional word vectors**.
* Capture **semantic similarity & analogical relationships**.
* Learned using **Word2Vec, GloVe, FastText**.
* Widely used in **IR, MT, QA, classification**.


## References

* [Word Embedding](https://en.wikipedia.org/wiki/Word_embedding)
* [Distributional Semantics](https://en.wikipedia.org/wiki/Distributional_semantics)
* [Word2Vec](https://en.wikipedia.org/wiki/Word2vec)
* [GloVe](https://en.wikipedia.org/wiki/GloVe_%28machine_learning%29)
* [FastText](https://en.wikipedia.org/wiki/FastText)


# Natural Language Processing â€“ Lecture 21

## ğŸ§® Word2Vec: Neural Word Embeddings

This lecture explains **Word2Vec**, a widely used method for learning **dense word embeddings** using shallow neural networks.


## ğŸ“– Word2Vec Overview

* Introduced by **Mikolov et al. (2013)** at Google.
* Learns word vectors from large corpora using a **prediction-based approach**.
* Two architectures:

  1. **Continuous Bag-of-Words (CBOW)**
  2. **Skip-gram**

ğŸ“– [Word2Vec (Wikipedia)](https://en.wikipedia.org/wiki/Word2vec)


## âš™ï¸ CBOW Model

### Idea

* Predict the **target word** given its **context words**.

Example:

* Context: *â€œthe \_\_\_ barks loudlyâ€*
* Predict: *â€œdogâ€*

### Architecture

* Input: One-hot vectors for context words
* Hidden layer: Shared weight matrix â†’ word embeddings
* Output: Softmax over vocabulary â†’ predicted word


## âš™ï¸ Skip-Gram Model

### Idea

* Opposite of CBOW: predict **context words** given a **target word**.

Example:

* Input: *â€œdogâ€*
* Predict: *â€œthe, barks, loudlyâ€*

### Advantage

* Works better for **rare words**.


## ğŸ”¢ Training Word2Vec

### Objective Function

* Maximize probability of predicting correct words.
* Skip-gram with context window $c$:

$$
  \frac{1}{T} \sum_{t=1}^{T} \sum_{-c \leq j \leq c, j \neq 0} \log P(w_{t+j} | w_t)
$$


## âš¡ Efficiency Tricks

1. **Negative Sampling**

   * Instead of softmax over all words, sample a few **negative examples**.
   * Greatly speeds up training.

2. **Hierarchical Softmax**

   * Organize vocabulary as a binary tree.
   * Compute probabilities in $O(\log V)$ instead of $O(V)$.

ğŸ“– [Negative Sampling](https://en.wikipedia.org/wiki/Word2vec#Negative_sampling)


## ğŸ§© Properties of Word2Vec Embeddings

* Capture **semantic similarity** (cosine similarity of vectors).
* Capture **analogical reasoning**:

  * *king â€“ man + woman â‰ˆ queen*
* Cluster words by meaning (animals, professions, places).


## ğŸ“Š Applications

* **Semantic Search** â†’ query expansion using embeddings
* **Machine Translation** â†’ bilingual embeddings
* **Recommendation Systems** â†’ items treated as words
* **Text Classification** â†’ features for classifiers



## ğŸ“Œ Summary

* **Word2Vec** learns embeddings via **CBOW** (predict word from context) or **Skip-gram** (predict context from word).
* Uses **negative sampling** or **hierarchical softmax** for efficiency.
* Embeddings capture **semantic similarity and analogies**.
* Widely applied in NLP and beyond.



## References

* [Word2Vec](https://en.wikipedia.org/wiki/Word2vec)
* [Negative Sampling](https://en.wikipedia.org/wiki/Word2vec#Negative_sampling)
* [Neural Word Embeddings](https://en.wikipedia.org/wiki/Word_embedding)

